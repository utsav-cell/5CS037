{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b02e15",
   "metadata": {},
   "source": [
    "## Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129a9f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5011b343",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Regression Task (California Housing)\n",
    "\n",
    "In this section, we will:\n",
    "1. Load and split the California Housing dataset\n",
    "2. Build a baseline linear regression model\n",
    "3. Perform hyperparameter tuning for Ridge and Lasso\n",
    "4. Compare L1 vs L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769093a3",
   "metadata": {},
   "source": [
    "## Task 1: Load and Split Dataset\n",
    "\n",
    "### About the Dataset\n",
    "The California Housing dataset contains information from the 1990 California census. It includes:\n",
    "- **8 features**: MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude\n",
    "- **Target**: Median house value for California districts (in $100,000s)\n",
    "- **Samples**: 20,640 observations\n",
    "\n",
    "We will split the data into:\n",
    "- **Training set**: 80% (for model training)\n",
    "- **Test set**: 20% (for model evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ceae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California Housing Dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"Target variable range: [{y.min():.2f}, {y.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe24a7c",
   "metadata": {},
   "source": [
    "### Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d43e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for better visualization\n",
    "california_data = fetch_california_housing()\n",
    "df_california = pd.DataFrame(X_train, columns=california_data.feature_names)\n",
    "df_california['Target'] = y_train\n",
    "\n",
    "print(\"Dataset Info:\")\n",
    "print(df_california.head())\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df_california.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77c02be",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "**Important**: For regularization techniques (Ridge and Lasso), feature scaling is crucial because:\n",
    "- Regularization penalizes large coefficients\n",
    "- Features with different scales will be penalized differently\n",
    "- Standardization ensures fair comparison and better convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c947b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature scaling completed!\")\n",
    "print(f\"Mean of scaled features: {X_train_scaled.mean(axis=0).round(10)}\")\n",
    "print(f\"Std of scaled features: {X_train_scaled.std(axis=0).round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6662c315",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Step 1 - Baseline Model (No Regularization)\n",
    "\n",
    "### Linear Regression Without Regularization\n",
    "\n",
    "We start with a basic Linear Regression model to establish a baseline. This model minimizes:\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**Key Points:**\n",
    "- No regularization term\n",
    "- May overfit if features are correlated or dataset is noisy\n",
    "- Serves as a baseline for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a572ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build baseline Linear Regression model\n",
    "baseline_model = LinearRegression()\n",
    "baseline_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_baseline = baseline_model.predict(X_train_scaled)\n",
    "y_test_pred_baseline = baseline_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate MSE\n",
    "train_mse_baseline = mean_squared_error(y_train, y_train_pred_baseline)\n",
    "test_mse_baseline = mean_squared_error(y_test, y_test_pred_baseline)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE MODEL (No Regularization)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training MSE: {train_mse_baseline:.4f}\")\n",
    "print(f\"Test MSE: {test_mse_baseline:.4f}\")\n",
    "print(f\"\\nR² Score (Train): {baseline_model.score(X_train_scaled, y_train):.4f}\")\n",
    "print(f\"R² Score (Test): {baseline_model.score(X_test_scaled, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f79401f",
   "metadata": {},
   "source": [
    "### Observing Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bb0de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display coefficients\n",
    "feature_names = california_data.feature_names\n",
    "coefficients_baseline = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': baseline_model.coef_\n",
    "}).sort_values('Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nBaseline Model Coefficients:\")\n",
    "print(coefficients_baseline)\n",
    "\n",
    "# Visualize coefficients\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(coefficients_baseline['Feature'], coefficients_baseline['Coefficient'])\n",
    "plt.xlabel('Coefficient Value', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.title('Baseline Model - Feature Coefficients', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39db13",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Step 2 - Hyperparameter Tuning\n",
    "\n",
    "### Ridge Regression (L2 Regularization)\n",
    "\n",
    "Ridge regression adds an L2 penalty term:\n",
    "\n",
    "$$\\text{Loss} = \\text{MSE} + \\alpha \\sum_{j=1}^{p} w_j^2$$\n",
    "\n",
    "where $\\alpha$ is the regularization strength.\n",
    "\n",
    "**Characteristics:**\n",
    "- Shrinks all coefficients towards zero\n",
    "- Does not set coefficients exactly to zero\n",
    "- Good when all features are potentially relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0361b1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for Ridge\n",
    "param_grid_ridge = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "# Initialize Ridge model\n",
    "ridge_model = Ridge()\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning\n",
    "grid_search_ridge = GridSearchCV(\n",
    "    estimator=ridge_model,\n",
    "    param_grid=param_grid_ridge,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training Ridge Regression with GridSearchCV...\")\n",
    "grid_search_ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RIDGE REGRESSION - Hyperparameter Tuning Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best alpha: {grid_search_ridge.best_params_['alpha']}\")\n",
    "print(f\"Best CV MSE: {-grid_search_ridge.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a2bb62",
   "metadata": {},
   "source": [
    "### Lasso Regression (L1 Regularization)\n",
    "\n",
    "Lasso regression adds an L1 penalty term:\n",
    "\n",
    "$$\\text{Loss} = \\text{MSE} + \\alpha \\sum_{j=1}^{p} |w_j|$$\n",
    "\n",
    "**Characteristics:**\n",
    "- Can set some coefficients exactly to zero\n",
    "- Performs feature selection automatically\n",
    "- Good when only a subset of features is relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d7f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for Lasso\n",
    "param_grid_lasso = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# Initialize Lasso model\n",
    "lasso_model = Lasso(max_iter=10000)\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning\n",
    "grid_search_lasso = GridSearchCV(\n",
    "    estimator=lasso_model,\n",
    "    param_grid=param_grid_lasso,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training Lasso Regression with GridSearchCV...\")\n",
    "grid_search_lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LASSO REGRESSION - Hyperparameter Tuning Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best alpha: {grid_search_lasso.best_params_['alpha']}\")\n",
    "print(f\"Best CV MSE: {-grid_search_lasso.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27278fbb",
   "metadata": {},
   "source": [
    "### Evaluating Best Models on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef3ff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best models\n",
    "best_ridge = grid_search_ridge.best_estimator_\n",
    "best_lasso = grid_search_lasso.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_ridge = best_ridge.predict(X_train_scaled)\n",
    "y_test_pred_ridge = best_ridge.predict(X_test_scaled)\n",
    "\n",
    "y_train_pred_lasso = best_lasso.predict(X_train_scaled)\n",
    "y_test_pred_lasso = best_lasso.predict(X_test_scaled)\n",
    "\n",
    "# Calculate MSE\n",
    "train_mse_ridge = mean_squared_error(y_train, y_train_pred_ridge)\n",
    "test_mse_ridge = mean_squared_error(y_test, y_test_pred_ridge)\n",
    "\n",
    "train_mse_lasso = mean_squared_error(y_train, y_train_pred_lasso)\n",
    "test_mse_lasso = mean_squared_error(y_test, y_test_pred_lasso)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Model':<20} {'Train MSE':<15} {'Test MSE':<15}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Baseline':<20} {train_mse_baseline:<15.4f} {test_mse_baseline:<15.4f}\")\n",
    "print(f\"{'Ridge':<20} {train_mse_ridge:<15.4f} {test_mse_ridge:<15.4f}\")\n",
    "print(f\"{'Lasso':<20} {train_mse_lasso:<15.4f} {test_mse_lasso:<15.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cce9824",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Step 3 - Regularization Experiments (L1 vs L2)\n",
    "\n",
    "### Comparing Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e296e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "coef_comparison = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Baseline': baseline_model.coef_,\n",
    "    'Ridge': best_ridge.coef_,\n",
    "    'Lasso': best_lasso.coef_\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COEFFICIENT COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(coef_comparison)\n",
    "\n",
    "# Count zero coefficients in Lasso\n",
    "zero_coef_lasso = np.sum(np.abs(best_lasso.coef_) < 1e-10)\n",
    "print(f\"\\nNumber of features with zero coefficient in Lasso: {zero_coef_lasso}/{len(feature_names)}\")\n",
    "print(f\"Lasso performed feature selection by eliminating {zero_coef_lasso} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8537a9",
   "metadata": {},
   "source": [
    "### Visualizing Coefficient Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc01541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot coefficient comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "models = ['Baseline', 'Ridge', 'Lasso']\n",
    "for idx, (ax, model) in enumerate(zip(axes, models)):\n",
    "    coef_data = coef_comparison.sort_values(model, ascending=False)\n",
    "    ax.barh(coef_data['Feature'], coef_data[model])\n",
    "    ax.set_xlabel('Coefficient Value', fontsize=11)\n",
    "    ax.set_ylabel('Features', fontsize=11)\n",
    "    ax.set_title(f'{model} Coefficients', fontsize=13, fontweight='bold')\n",
    "    ax.axvline(x=0, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012e91a6",
   "metadata": {},
   "source": [
    "### Effect of Alpha on Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb61ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different alpha values\n",
    "alphas = np.logspace(-3, 3, 50)\n",
    "train_errors_ridge = []\n",
    "test_errors_ridge = []\n",
    "train_errors_lasso = []\n",
    "test_errors_lasso = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Ridge\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train_scaled, y_train)\n",
    "    train_errors_ridge.append(mean_squared_error(y_train, ridge.predict(X_train_scaled)))\n",
    "    test_errors_ridge.append(mean_squared_error(y_test, ridge.predict(X_test_scaled)))\n",
    "    \n",
    "    # Lasso\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "    train_errors_lasso.append(mean_squared_error(y_train, lasso.predict(X_train_scaled)))\n",
    "    test_errors_lasso.append(mean_squared_error(y_test, lasso.predict(X_test_scaled)))\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Ridge\n",
    "ax1.plot(alphas, train_errors_ridge, label='Train MSE', linewidth=2)\n",
    "ax1.plot(alphas, test_errors_ridge, label='Test MSE', linewidth=2)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Alpha (Regularization Strength)', fontsize=12)\n",
    "ax1.set_ylabel('Mean Squared Error', fontsize=12)\n",
    "ax1.set_title('Ridge Regression: MSE vs Alpha', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Lasso\n",
    "ax2.plot(alphas, train_errors_lasso, label='Train MSE', linewidth=2)\n",
    "ax2.plot(alphas, test_errors_lasso, label='Test MSE', linewidth=2)\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_xlabel('Alpha (Regularization Strength)', fontsize=12)\n",
    "ax2.set_ylabel('Mean Squared Error', fontsize=12)\n",
    "ax2.set_title('Lasso Regression: MSE vs Alpha', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad75dad",
   "metadata": {},
   "source": [
    "### Discussion: Bias-Variance Tradeoff\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "1. **L1 (Lasso) Regularization:**\n",
    "   - Produces sparse models by setting some coefficients to exactly zero\n",
    "   - Performs automatic feature selection\n",
    "   - Useful when we believe only a subset of features is relevant\n",
    "   - Can lead to better interpretability\n",
    "\n",
    "2. **L2 (Ridge) Regularization:**\n",
    "   - Shrinks all coefficients towards zero but rarely sets them exactly to zero\n",
    "   - Handles multicollinearity well\n",
    "   - Useful when all features are potentially relevant\n",
    "   - More stable than Lasso when features are correlated\n",
    "\n",
    "3. **Effect on Bias-Variance:**\n",
    "   - **Low alpha (weak regularization)**: Lower bias, higher variance → potential overfitting\n",
    "   - **High alpha (strong regularization)**: Higher bias, lower variance → potential underfitting\n",
    "   - **Optimal alpha**: Balances bias and variance for best generalization\n",
    "\n",
    "4. **Cross-Validation:**\n",
    "   - Helps find the optimal regularization strength\n",
    "   - Ensures the model generalizes well to unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118c5298",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Classification Task (Breast Cancer)\n",
    "\n",
    "In this section, we will:\n",
    "1. Load and split the Breast Cancer dataset\n",
    "2. Build a baseline logistic regression model\n",
    "3. Perform hyperparameter tuning for L1 and L2 regularization\n",
    "4. Compare L1 vs L2 regularization for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf3502a",
   "metadata": {},
   "source": [
    "## Task 1: Load and Split Dataset\n",
    "\n",
    "### About the Dataset\n",
    "The Breast Cancer Wisconsin dataset contains features computed from digitized images of breast mass:\n",
    "- **30 features**: Mean, standard error, and worst values for 10 characteristics\n",
    "- **Target**: Binary classification (0 = malignant, 1 = benign)\n",
    "- **Samples**: 569 observations\n",
    "\n",
    "We will split the data into:\n",
    "- **Training set**: 80%\n",
    "- **Test set**: 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8286228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Breast Cancer Dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(f\"Class 0 (Malignant): {np.sum(y_train == 0)} ({np.sum(y_train == 0)/len(y_train)*100:.1f}%)\")\n",
    "print(f\"Class 1 (Benign): {np.sum(y_train == 1)} ({np.sum(y_train == 1)/len(y_train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587d79ed",
   "metadata": {},
   "source": [
    "### Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3050433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "cancer_data = load_breast_cancer()\n",
    "df_cancer = pd.DataFrame(X_train, columns=cancer_data.feature_names)\n",
    "df_cancer['Target'] = y_train\n",
    "\n",
    "print(\"Dataset Info:\")\n",
    "print(df_cancer.head())\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df_cancer.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e11d722",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2427c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler_cancer = StandardScaler()\n",
    "X_train_scaled_cancer = scaler_cancer.fit_transform(X_train)\n",
    "X_test_scaled_cancer = scaler_cancer.transform(X_test)\n",
    "\n",
    "print(\"Feature scaling completed!\")\n",
    "print(f\"Mean of scaled features: {X_train_scaled_cancer.mean(axis=0).round(10)[0]}\")\n",
    "print(f\"Std of scaled features: {X_train_scaled_cancer.std(axis=0).round(2)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80884185",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Step 1 - Baseline Model (No Regularization)\n",
    "\n",
    "### Logistic Regression Without Regularization\n",
    "\n",
    "Logistic regression predicts the probability of a binary outcome:\n",
    "\n",
    "$$P(y=1|x) = \\frac{1}{1 + e^{-(w^Tx + b)}}$$\n",
    "\n",
    "The model minimizes the log loss (binary cross-entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfaa914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build baseline Logistic Regression model\n",
    "# Note: penalty='none' means no regularization\n",
    "baseline_logreg = LogisticRegression(penalty=None, max_iter=10000, random_state=42)\n",
    "baseline_logreg.fit(X_train_scaled_cancer, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_baseline = baseline_logreg.predict(X_train_scaled_cancer)\n",
    "y_test_pred_baseline = baseline_logreg.predict(X_test_scaled_cancer)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_acc_baseline = accuracy_score(y_train, y_train_pred_baseline)\n",
    "test_acc_baseline = accuracy_score(y_test, y_test_pred_baseline)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE MODEL (No Regularization)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training Accuracy: {train_acc_baseline:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_baseline:.4f}\")\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_test_pred_baseline, target_names=['Malignant', 'Benign']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab04635",
   "metadata": {},
   "source": [
    "### Observing Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a26568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display coefficients\n",
    "feature_names_cancer = cancer_data.feature_names\n",
    "coefficients_baseline_cancer = pd.DataFrame({\n",
    "    'Feature': feature_names_cancer,\n",
    "    'Coefficient': baseline_logreg.coef_[0]\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nBaseline Model - Top 10 Coefficients (by absolute value):\")\n",
    "print(coefficients_baseline_cancer.head(10))\n",
    "\n",
    "# Visualize top 15 coefficients\n",
    "top_15 = coefficients_baseline_cancer.head(15)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(top_15['Feature'], top_15['Coefficient'])\n",
    "plt.xlabel('Coefficient Value', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.title('Baseline Model - Top 15 Feature Coefficients', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72e765b",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c390a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm_baseline = confusion_matrix(y_test, y_test_pred_baseline)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Malignant', 'Benign'],\n",
    "            yticklabels=['Malignant', 'Benign'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.title('Confusion Matrix - Baseline Model', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e2e683",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Step 2 - Hyperparameter Tuning\n",
    "\n",
    "### Understanding the C Parameter\n",
    "\n",
    "In sklearn's LogisticRegression:\n",
    "- **C** is the inverse of regularization strength\n",
    "- **Smaller C** = stronger regularization\n",
    "- **Larger C** = weaker regularization\n",
    "\n",
    "Loss function with regularization:\n",
    "$$\\text{Loss} = \\text{Log Loss} + \\frac{1}{C} \\cdot \\text{Penalty}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7227ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid_logreg = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "logreg_model = LogisticRegression(solver='saga', max_iter=10000, random_state=42)\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning\n",
    "grid_search_logreg = GridSearchCV(\n",
    "    estimator=logreg_model,\n",
    "    param_grid=param_grid_logreg,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training Logistic Regression with GridSearchCV...\")\n",
    "grid_search_logreg.fit(X_train_scaled_cancer, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOGISTIC REGRESSION - Hyperparameter Tuning Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best parameters: {grid_search_logreg.best_params_}\")\n",
    "print(f\"Best CV accuracy: {grid_search_logreg.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa570ca8",
   "metadata": {},
   "source": [
    "### Separate Tuning for L1 and L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a280fa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 Regularization (Lasso-like)\n",
    "param_grid_l1 = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "logreg_l1 = LogisticRegression(penalty='l1', solver='saga', max_iter=10000, random_state=42)\n",
    "grid_search_l1 = GridSearchCV(logreg_l1, param_grid_l1, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_l1.fit(X_train_scaled_cancer, y_train)\n",
    "\n",
    "print(\"L1 Regularization:\")\n",
    "print(f\"Best C: {grid_search_l1.best_params_['C']}\")\n",
    "print(f\"Best CV accuracy: {grid_search_l1.best_score_:.4f}\")\n",
    "\n",
    "# L2 Regularization (Ridge-like)\n",
    "param_grid_l2 = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "logreg_l2 = LogisticRegression(penalty='l2', solver='saga', max_iter=10000, random_state=42)\n",
    "grid_search_l2 = GridSearchCV(logreg_l2, param_grid_l2, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_l2.fit(X_train_scaled_cancer, y_train)\n",
    "\n",
    "print(\"\\nL2 Regularization:\")\n",
    "print(f\"Best C: {grid_search_l2.best_params_['C']}\")\n",
    "print(f\"Best CV accuracy: {grid_search_l2.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240136cb",
   "metadata": {},
   "source": [
    "### Evaluating Best Models on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5f1ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best models\n",
    "best_logreg_l1 = grid_search_l1.best_estimator_\n",
    "best_logreg_l2 = grid_search_l2.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_l1 = best_logreg_l1.predict(X_train_scaled_cancer)\n",
    "y_test_pred_l1 = best_logreg_l1.predict(X_test_scaled_cancer)\n",
    "\n",
    "y_train_pred_l2 = best_logreg_l2.predict(X_train_scaled_cancer)\n",
    "y_test_pred_l2 = best_logreg_l2.predict(X_test_scaled_cancer)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_acc_l1 = accuracy_score(y_train, y_train_pred_l1)\n",
    "test_acc_l1 = accuracy_score(y_test, y_test_pred_l1)\n",
    "\n",
    "train_acc_l2 = accuracy_score(y_train, y_train_pred_l2)\n",
    "test_acc_l2 = accuracy_score(y_test, y_test_pred_l2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Model':<25} {'Train Accuracy':<20} {'Test Accuracy':<20}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Baseline (No Reg)':<25} {train_acc_baseline:<20.4f} {test_acc_baseline:<20.4f}\")\n",
    "print(f\"{'L1 Regularization':<25} {train_acc_l1:<20.4f} {test_acc_l1:<20.4f}\")\n",
    "print(f\"{'L2 Regularization':<25} {train_acc_l2:<20.4f} {test_acc_l2:<20.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3955b1c",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Step 3 - Regularization Experiments (L1 vs L2)\n",
    "\n",
    "### Comparing Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe68f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "coef_comparison_cancer = pd.DataFrame({\n",
    "    'Feature': feature_names_cancer,\n",
    "    'Baseline': baseline_logreg.coef_[0],\n",
    "    'L1': best_logreg_l1.coef_[0],\n",
    "    'L2': best_logreg_l2.coef_[0]\n",
    "})\n",
    "\n",
    "# Sort by absolute value of L1 coefficient\n",
    "coef_comparison_cancer['L1_abs'] = np.abs(coef_comparison_cancer['L1'])\n",
    "coef_comparison_cancer = coef_comparison_cancer.sort_values('L1_abs', ascending=False)\n",
    "coef_comparison_cancer = coef_comparison_cancer.drop('L1_abs', axis=1)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COEFFICIENT COMPARISON - Top 15 Features\")\n",
    "print(\"=\" * 80)\n",
    "print(coef_comparison_cancer.head(15))\n",
    "\n",
    "# Count zero coefficients in L1\n",
    "zero_coef_l1 = np.sum(np.abs(best_logreg_l1.coef_[0]) < 1e-10)\n",
    "print(f\"\\nNumber of features with zero coefficient in L1: {zero_coef_l1}/{len(feature_names_cancer)}\")\n",
    "print(f\"L1 regularization performed feature selection by eliminating {zero_coef_l1} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bc6c97",
   "metadata": {},
   "source": [
    "### Visualizing Coefficient Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272675ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot coefficient comparison for top 15 features\n",
    "top_15_cancer = coef_comparison_cancer.head(15)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "models_cancer = ['Baseline', 'L1', 'L2']\n",
    "\n",
    "for idx, (ax, model) in enumerate(zip(axes, models_cancer)):\n",
    "    ax.barh(top_15_cancer['Feature'], top_15_cancer[model])\n",
    "    ax.set_xlabel('Coefficient Value', fontsize=11)\n",
    "    ax.set_ylabel('Features', fontsize=11)\n",
    "    ax.set_title(f'{model} Coefficients (Top 15)', fontsize=13, fontweight='bold')\n",
    "    ax.axvline(x=0, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e9478a",
   "metadata": {},
   "source": [
    "### Effect of C on Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed4a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different C values\n",
    "C_values = np.logspace(-3, 3, 50)\n",
    "train_acc_l1_list = []\n",
    "test_acc_l1_list = []\n",
    "train_acc_l2_list = []\n",
    "test_acc_l2_list = []\n",
    "n_features_l1 = []\n",
    "\n",
    "for C in C_values:\n",
    "    # L1\n",
    "    logreg_l1_temp = LogisticRegression(penalty='l1', C=C, solver='saga', max_iter=10000, random_state=42)\n",
    "    logreg_l1_temp.fit(X_train_scaled_cancer, y_train)\n",
    "    train_acc_l1_list.append(accuracy_score(y_train, logreg_l1_temp.predict(X_train_scaled_cancer)))\n",
    "    test_acc_l1_list.append(accuracy_score(y_test, logreg_l1_temp.predict(X_test_scaled_cancer)))\n",
    "    n_features_l1.append(np.sum(np.abs(logreg_l1_temp.coef_[0]) > 1e-10))\n",
    "    \n",
    "    # L2\n",
    "    logreg_l2_temp = LogisticRegression(penalty='l2', C=C, solver='saga', max_iter=10000, random_state=42)\n",
    "    logreg_l2_temp.fit(X_train_scaled_cancer, y_train)\n",
    "    train_acc_l2_list.append(accuracy_score(y_train, logreg_l2_temp.predict(X_train_scaled_cancer)))\n",
    "    test_acc_l2_list.append(accuracy_score(y_test, logreg_l2_temp.predict(X_test_scaled_cancer)))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# L1\n",
    "axes[0].plot(C_values, train_acc_l1_list, label='Train Accuracy', linewidth=2)\n",
    "axes[0].plot(C_values, test_acc_l1_list, label='Test Accuracy', linewidth=2)\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xlabel('C (Inverse Regularization Strength)', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('L1 Regularization: Accuracy vs C', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# L2\n",
    "axes[1].plot(C_values, train_acc_l2_list, label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(C_values, test_acc_l2_list, label='Test Accuracy', linewidth=2)\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_xlabel('C (Inverse Regularization Strength)', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('L2 Regularization: Accuracy vs C', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Number of features selected by L1\n",
    "axes[2].plot(C_values, n_features_l1, linewidth=2, color='green')\n",
    "axes[2].set_xscale('log')\n",
    "axes[2].set_xlabel('C (Inverse Regularization Strength)', fontsize=12)\n",
    "axes[2].set_ylabel('Number of Non-Zero Features', fontsize=12)\n",
    "axes[2].set_title('L1: Feature Selection vs C', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].axhline(y=len(feature_names_cancer), color='red', linestyle='--', label='Total Features')\n",
    "axes[2].legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5757433b",
   "metadata": {},
   "source": [
    "### Confusion Matrices Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62186ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrices\n",
    "cm_baseline = confusion_matrix(y_test, y_test_pred_baseline)\n",
    "cm_l1 = confusion_matrix(y_test, y_test_pred_l1)\n",
    "cm_l2 = confusion_matrix(y_test, y_test_pred_l2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "cms = [cm_baseline, cm_l1, cm_l2]\n",
    "titles = ['Baseline (No Reg)', 'L1 Regularization', 'L2 Regularization']\n",
    "\n",
    "for ax, cm, title in zip(axes, cms, titles):\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Malignant', 'Benign'],\n",
    "                yticklabels=['Malignant', 'Benign'],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    ax.set_xlabel('Predicted Label', fontsize=11)\n",
    "    ax.set_ylabel('True Label', fontsize=11)\n",
    "    ax.set_title(f'Confusion Matrix - {title}', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7139729",
   "metadata": {},
   "source": [
    "### Discussion: Bias-Variance Tradeoff in Classification\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "1. **L1 (Lasso-like) Regularization:**\n",
    "   - Sets some coefficients to exactly zero\n",
    "   - Automatically performs feature selection\n",
    "   - More interpretable models with fewer features\n",
    "   - Can improve generalization by reducing model complexity\n",
    "\n",
    "2. **L2 (Ridge-like) Regularization:**\n",
    "   - Shrinks all coefficients but rarely to zero\n",
    "   - Keeps all features but with reduced impact\n",
    "   - More stable when features are correlated\n",
    "   - Generally provides smooth coefficient distributions\n",
    "\n",
    "3. **Effect on Bias-Variance:**\n",
    "   - **Large C (weak regularization)**: Model can overfit → high variance, low bias\n",
    "   - **Small C (strong regularization)**: Model may underfit → low variance, high bias\n",
    "   - **Optimal C**: Balances complexity and fit for best test performance\n",
    "\n",
    "4. **Practical Insights:**\n",
    "   - Use L1 when you want feature selection and interpretability\n",
    "   - Use L2 when all features are potentially important\n",
    "   - Cross-validation is essential for finding optimal regularization\n",
    "   - Both methods help prevent overfitting and improve generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e1c4cc",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### Part 1: Regression (California Housing)\n",
    "- Ridge and Lasso both improved generalization compared to baseline\n",
    "- Lasso performed automatic feature selection\n",
    "- Optimal alpha found through cross-validation\n",
    "- Regularization reduced overfitting and improved test performance\n",
    "\n",
    "### Part 2: Classification (Breast Cancer)\n",
    "- Both L1 and L2 regularization improved model performance\n",
    "- L1 created sparser models with fewer features\n",
    "- L2 kept all features but with controlled magnitudes\n",
    "- Cross-validation helped identify optimal hyperparameters\n",
    "\n",
    "### General Principles:\n",
    "1. **Always standardize features** before applying regularization\n",
    "2. **Use cross-validation** to find optimal hyperparameters\n",
    "3. **L1 for feature selection**, L2 for stability\n",
    "4. **Monitor train vs test performance** to detect over/underfitting\n",
    "5. **Regularization is a powerful tool** for improving generalization\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This worksheet demonstrated the practical application of regularization techniques in both regression and classification tasks. We learned how to:\n",
    "- Apply Ridge (L2) and Lasso (L1) regularization\n",
    "- Tune hyperparameters using GridSearchCV\n",
    "- Understand and visualize the bias-variance tradeoff\n",
    "- Compare model performance across different regularization strategies\n",
    "\n",
    "These techniques are fundamental in machine learning and essential for building robust, generalizable models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
